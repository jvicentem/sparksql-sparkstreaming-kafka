{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL assignment with Python\n",
    "## José Vicente Mellado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## No need to run these lines, everything has been configured in spark-env.sh\n",
    "\n",
    "#import findspark\n",
    "#findspark.init('/spark_dir')\n",
    "\n",
    "##Configuramos el sparksession\n",
    "#import pyspark\n",
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "#spark = (SparkSession.builder\n",
    "#         .master('local[*]')\n",
    "#         .config('spark.driver.cores', 1)\n",
    "#         .appName('estudio_spark')\n",
    "#         .getOrCreate()\n",
    "#        )\n",
    "##obtenemos el sparkcontext a partir del sparksession\n",
    "#sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "st = StructType([\n",
    "        StructField('ID', LongType(), True),\n",
    "        StructField('PARENT-SYS-ID', StringType(), True),\n",
    "        StructField('Source', StringType(), True),\n",
    "        StructField('Mentions', StringType(), True),\n",
    "        StructField('Target', StringType(), True),\n",
    "        StructField('NAME Source', StringType(), True),\n",
    "        StructField('BODY', StringType(), True),\n",
    "        StructField('PUBDATE', TimestampType(), True),\n",
    "        StructField('URLs coma separated', StringType(), True),\n",
    "        StructField('Type TW-RT-MT', StringType(), True),\n",
    "        StructField('LINK', StringType(), True),\n",
    "        StructField('n1 Link', ByteType(), True),\n",
    "        StructField('n1 Picture', ByteType(), True),\n",
    "        StructField('PERSONAL-WEBSITE', StringType(), True),\n",
    "        StructField('COUNTRY', StringType(), True),\n",
    "        StructField('ALL-NICK-ACTIVITY-EVER', LongType(), True),\n",
    "        StructField('NICK-FOLLOWERS', LongType(), True),\n",
    "        StructField('FRIENDS-FOLLOWING-AUDIENCE', LongType(), True),\n",
    "        StructField('LOCATION', StringType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "#https://spark.apache.org/docs/2.0.0-preview/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader\n",
    "df = spark.read.csv('tweets.csv', \n",
    "                    header=True, \n",
    "                    schema=st,\n",
    "                    timestampFormat='dd/MM/yyyy HH:mm',\n",
    "                    delimiter='\\t',\n",
    "                    mode='PERMISSIVE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def list_to_string(l):\n",
    "    if l != None:\n",
    "        return str(l).replace('[','').replace(']','').replace('\\'','').replace(' ', '')\n",
    "\n",
    "def remove_duplicates(list_string):\n",
    "    if list_string != None:\n",
    "        return list(filter(lambda x: len(x)>0, list(set(list_string.split(',')))))\n",
    "\n",
    "only_uniques = udf(lambda row: list_to_string(remove_duplicates( row )) if row != None else None, StringType())\n",
    "\n",
    "\n",
    "df = df.withColumn('Mentions', \n",
    "                    only_uniques(df['Mentions'])\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Contabilizar el número total de menciones a los pilotos Marc Márquez, Valentino Rossi y Dani Pedrosa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      Accounts|count|\n",
      "+--------------+-----+\n",
      "| marcmarquez93|58027|\n",
      "|26_danipedrosa|12341|\n",
      "|  valeyellow46|61103|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "(df.select('Mentions')\n",
    " .filter(df['Mentions'].like('%marcmarquez93%') | \n",
    "         df['Mentions'].like('%valeyellow46%') | \n",
    "         df['Mentions'].like('%26_danipedrosa%'))\n",
    " .select(explode((split('Mentions', ','))).alias('Accounts'))\n",
    " .filter('''Accounts = 'marcmarquez93' or \n",
    "            Accounts = 'valeyellow46' or \n",
    "            Accounts = '26_danipedrosa'\n",
    "         ''')\n",
    " .groupBy('Accounts')\n",
    " .count()\n",
    " .show()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Contabilizar los 5 países que más tweets han publicado (considerando los tweets que contengan dicha información).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(COUNTRY='es', count=172577),\n",
       " Row(COUNTRY='us', count=12722),\n",
       " Row(COUNTRY='gb', count=12588),\n",
       " Row(COUNTRY='id', count=8725),\n",
       " Row(COUNTRY='it', count=1843)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.select('COUNTRY')\n",
    " .filter(df['COUNTRY'] != 'not public')\n",
    " .groupBy('COUNTRY')\n",
    " .count()\n",
    " .orderBy('count', ascending=False)\n",
    " .take(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Contabilizar los 3 hashtags más utilizados (que aparezcan el mayor número de veces) en el cuerpo de los tweets (campo \"body\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Words='#motogp', count=51961),\n",
       " Row(Words='#qatar', count=9977),\n",
       " Row(Words='#moto3', count=5797)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.select('BODY')\n",
    " .filter(df['BODY'].like('%#%'))\n",
    " .select(explode((split('BODY', ' '))).alias('Words'))\n",
    " .filter(\"Words like '#%'\")\n",
    " .groupBy('Words')\n",
    " .count()\n",
    " .orderBy('count', ascending=False)\n",
    " .take(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please run first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following lines were provided by the teachers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from operator import add\n",
    "from operator import sub\n",
    "\n",
    "# Crear el contexto de Spark Streaming\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "kafkaBrokerIPPort = \"127.0.0.1:9092\"\n",
    "\n",
    "import kafka\n",
    "\n",
    "class KafkaProducerWrapper(object):\n",
    "  producer = None\n",
    "  @staticmethod\n",
    "  def getProducer(brokerList):\n",
    "    if KafkaProducerWrapper.producer != None:\n",
    "      return KafkaProducerWrapper.producer\n",
    "    else:\n",
    "      KafkaProducerWrapper.producer = kafka.KafkaProducer(bootstrap_servers=brokerList, key_serializer=str.encode, value_serializer=str.encode)\n",
    "      return KafkaProducerWrapper.producer\n",
    " \n",
    "def sendMetrics(itr):\n",
    "  prod = KafkaProducerWrapper.getProducer([kafkaBrokerIPPort])\n",
    "  for m in itr:\n",
    "    prod.send(\"metrics\", key=m[0], value=m[0]+\",\"+str(m[1]))\n",
    "  prod.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_name = 'Quatar_GP_2014'\n",
    "\n",
    "kafkaParams = {\"metadata.broker.list\": kafkaBrokerIPPort}\n",
    "stream = KafkaUtils.createDirectStream(ssc, [topic_name], kafkaParams)\n",
    "stream = stream.map(lambda o: str(o[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Calcular el número total de menciones recibidas por cada cuenta de usuario durante el intervalo de 5 segundos.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [intro]",
   "language": "python",
   "name": "Python [intro]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
